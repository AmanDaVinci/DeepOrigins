{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebuilding the PyTorch training loop in simple Python\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Rebuilding-the-PyTorch-training-loop-in-simple-Python\" data-toc-modified-id=\"Rebuilding-the-PyTorch-training-loop-in-simple-Python-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Rebuilding the PyTorch training loop in simple Python</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Import Libraries</a></span></li><li><span><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Load Data</a></span></li><li><span><a href=\"#Pre-process-Data\" data-toc-modified-id=\"Pre-process-Data-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Pre-process Data</a></span></li></ul></li><li><span><a href=\"#Bare-Training-Loop\" data-toc-modified-id=\"Bare-Training-Loop-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Bare Training Loop</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-Model\" data-toc-modified-id=\"Build-Model-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Build Model</a></span></li><li><span><a href=\"#Train-Model\" data-toc-modified-id=\"Train-Model-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Train Model</a></span></li><li><span><a href=\"#Evaluate-Model\" data-toc-modified-id=\"Evaluate-Model-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Evaluate Model</a></span></li></ul></li><li><span><a href=\"#PyTorch-Data-Abstractions\" data-toc-modified-id=\"PyTorch-Data-Abstractions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>PyTorch Data Abstractions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Dataset</a></span></li><li><span><a href=\"#Dataloader\" data-toc-modified-id=\"Dataloader-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Dataloader</a></span></li><li><span><a href=\"#Random-Sampler\" data-toc-modified-id=\"Random-Sampler-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Random Sampler</a></span></li></ul></li><li><span><a href=\"#PyTorch-Training-Abstractions\" data-toc-modified-id=\"PyTorch-Training-Abstractions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>PyTorch Training Abstractions</a></span><ul class=\"toc-item\"><li><span><a href=\"#nn.Parameters\" data-toc-modified-id=\"nn.Parameters-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>nn.Parameters</a></span></li><li><span><a href=\"#nn.Sequential\" data-toc-modified-id=\"nn.Sequential-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>nn.Sequential</a></span></li><li><span><a href=\"#Optimizer\" data-toc-modified-id=\"Optimizer-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Optimizer</a></span></li></ul></li><li><span><a href=\"#Validation\" data-toc-modified-id=\"Validation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Validation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(root=\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 28, 28]), torch.Size([10000, 28, 28]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = dataset.data.float(), dataset.targets\n",
    "x_train, x_test = x[:50000], x[50000:]\n",
    "y_train, y_test = y[:50000], y[50000:]\n",
    "\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7b45f87310>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flatten**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2])\n",
    "x_test = x_test.reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, mean, std): \n",
    "    return (x-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = x_train.mean()\n",
    "train_std = x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = normalize(x_train, train_mean, train_std)\n",
    "x_test = normalize(x_test, train_mean, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7b14147e50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAN9ElEQVR4nO3df6zddX3H8derP4FStIXSdrQOZY2KOoq5gUWKkxENsCG4BUKzsC4jq1sgyuLcGiCTZFvEOXUmU0iRhkoQ4lRCzZqN7oaEGV3TC5a20AEVK5aWVu20BbQ/3/vjnppLud/PvZzv9/zofT8fyc055/s+3/N956Svfs85n+/3+3FECMDEN6nXDQDoDsIOJEHYgSQIO5AEYQeSmNLNjU3z9DhJM7q5SSCVX+kVHYwDHq1WK+y2L5P0RUmTJX0lIu4oPf8kzdCFvrTOJgEUrI/BylrbH+NtT5b0JUmXSzpX0lLb57b7egA6q8539gskbYuI5yPioKQHJV3VTFsAmlYn7GdJ+vGIxztay17D9nLbQ7aHDulAjc0BqKNO2Ef7EeB1x95GxMqIGIiIgamaXmNzAOqoE/YdkhaOeLxA0s567QDolDph3yBpke232p4m6TpJa5ppC0DT2h56i4jDtm+S9J8aHnpbFRFPNdYZgEbVGmePiLWS1jbUC4AO4nBZIAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Lo6pTNaM/Ri88v1nfefLCytmTh88V1503fV6yv+/TFxfqBN406O/Cvzf3GM5W1Iz/bW1wXzWLPDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJOCK6trHTPDsu9KVd296JYvKsWcX6Z79fnij3HVOnN9lOo9b98uTK2m2f+bPiuqff/b2m25nw1seg9sXeUQ9+qHVQje3tkvZLOiLpcEQM1Hk9AJ3TxBF0l0TETxt4HQAdxHd2IIm6YQ9Jj9h+3Pby0Z5ge7ntIdtDh3Sg5uYAtKvux/iLImKn7TMlrbP9vxHx2MgnRMRKSSul4R/oam4PQJtq7dkjYmfrdo+khyRd0ERTAJrXdthtz7A989h9SR+StKWpxgA0q87H+LmSHrJ97HW+FhH/0UhX2UwqnxP+pZ9cUqxv/fncytoLm+cX133Le3YV65fOrT4fXZL+YOaTxfp5016trP3NJ79WXHf1ut8t1g9vf6FYx2u1HfaIeF7SeQ32AqCDGHoDkiDsQBKEHUiCsANJEHYgCU5xRS1TFpxVrD99W3V925V3Fdd972dvKtbn/ct3i/WMSqe4smcHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSSYshm1HN7xYrE+53tvqS5eWX7tfb9dPRW1JM0rr47jsGcHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQYZ0ctU+ZVX8Zaki7+2Pq2X3vuvJ+3vS5ejz07kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBODuKjl58frF+zd3/XqxfP/Olyto9+xYU1539V8WyjpTLOM6Ye3bbq2zvsb1lxLLZttfZfq51O6uzbQKoazwf4++VdNlxy1ZIGoyIRZIGW48B9LExwx4Rj0nae9ziqyStbt1fLenqZtsC0LR2f6CbGxG7JKl1e2bVE20vtz1ke+iQDrS5OQB1dfzX+IhYGREDETEwVdM7vTkAFdoN+27b8yWpdbunuZYAdEK7YV8jaVnr/jJJDzfTDoBOGXOc3fYDkj4g6QzbOyR9StIdkr5u+wZJL0i6ppNNonNeuvl9xfrf33hvsf77p7xcrO858mpl7b5byxeOP+WZ9s+Fx+uNGfaIWFpRurThXgB0EIfLAkkQdiAJwg4kQdiBJAg7kASnuE4Ak2dVn3T4zN+9vbju09d+sVifosnF+uaDh4r1Fdf+ZWXtlA0MrXUTe3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJx9gngFw9Uj7M/+54vj7F2eRz9oievLdZP+tfyhYWnb9gwxvbRLezZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtkngMt/4+mOvfbUr5xerE9fyznpJwr27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQhCOiaxs7zbPjQjP5a9OeveuCytq2K++q9doH4nCx/u7/qr4uvCS94x/2VtaObPthWz2h2voY1L7Y69FqY+7Zba+yvcf2lhHLbrf9ou2Nrb8rmmwYQPPG8zH+XkmXjbL8CxGxuPW3ttm2ADRtzLBHxGOSqj+LATgh1PmB7ibbm1of8ysvRGZ7ue0h20OHdKDG5gDU0W7Y75R0jqTFknZJ+lzVEyNiZUQMRMTAVE1vc3MA6mor7BGxOyKORMRRSXdLqv45GEBfaCvstuePePgRSVuqngugP4w5zm77AUkfkHSGpN2SPtV6vFhSSNou6aMRsWusjTHO3hmTZs6srO3/tznFdf/6nEeK9StP2ddWT8f896+qL5lwy63Li+vOfPB/am07o9I4+5gXr4iIpaMsvqd2VwC6isNlgSQIO5AEYQeSIOxAEoQdSIJTXCe4STNmFOueNq1Y//aWwSbbeY2fHf1lsX7Jlz9ZrC/49HebbGdCqHWKK4CJgbADSRB2IAnCDiRB2IEkCDuQBGEHkmCcHUVHlywu1ud85kfF+n1ntz9O/+1XTyvW71z0W22/9kTFODsAwg5kQdiBJAg7kARhB5Ig7EAShB1IgnH2PjD5tPJ48pF99S7n3ElT5s0t1l/56smVtcF3favWtj988R8W64ef317r9U9EjLMDIOxAFoQdSIKwA0kQdiAJwg4kQdiBJMacxRX1TTrvncX6ioceKNb/fMOflF9/66mVtZNfKh9H8bY/fq5YP2XKwWL992Z9v1i/fuZLxXrJ/fvPLNYzjqPXMeae3fZC24/a3mr7Kdsfby2fbXud7edat7M63y6Ado3nY/xhSZ+IiHdK+h1JN9o+V9IKSYMRsUjSYOsxgD41ZtgjYldEPNG6v1/SVklnSbpK0urW01ZLurpDPQJowBv6gc722ZLOl7Re0tyI2CUN/4cgadQvWLaX2x6yPXRIB2q2C6Bd4w677VMlfVPSzREx7jMzImJlRAxExMBUTW+nRwANGFfYbU/VcNDvj4hjpyrttj2/VZ8vaU9nWgTQhDGH3mxb0j2StkbE50eU1khaJumO1u3DHelwAvjB0jcX6+8/qbz+00vuLT9hyRtq5w2Z7PL+4Egcbfu1Xzj8arG+8rY/KtZnaH3b285oPOPsF0m6XtJm2xtby27RcMi/bvsGSS9IuqYjHQJoxJhhj4jvSBr1ZHhJXIkCOEFwuCyQBGEHkiDsQBKEHUiCsANJcIprFxyadbjXLXTMkk3lEddT/3FmZW3ai/9XXHfGDxlHbxJ7diAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2Lnj7xzYV6+979C+K9Veu+0Wx/q451Zdr3vHym4vrjuXoyvLlnN+0pnwp6ThUfSnqiXv0QX9izw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSTiiPKVvk07z7LjQXJAW6JT1Mah9sXfUq0GzZweSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJMYMu+2Fth+1vdX2U7Y/3lp+u+0XbW9s/V3R+XYBtGs8F684LOkTEfGE7ZmSHre9rlX7QkT8c+faA9CU8czPvkvSrtb9/ba3Sjqr040BaNYb+s5u+2xJ50s6Ni/PTbY32V5le1bFOsttD9keOqQD9boF0LZxh932qZK+KenmiNgn6U5J50harOE9/+dGWy8iVkbEQEQMTNX0+h0DaMu4wm57qoaDfn9EfEuSImJ3RByJiKOS7pZ0QefaBFDXeH6Nt6R7JG2NiM+PWD5/xNM+ImlL8+0BaMp4fo2/SNL1kjbb3thadoukpbYXSwpJ2yV9tAP9AWjIeH6N/46k0c6PXdt8OwA6hSPogCQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSXR1ymbbP5H0oxGLzpD006418Mb0a2/92pdEb+1qsrffjIg5oxW6GvbXbdweioiBnjVQ0K+99WtfEr21q1u98TEeSIKwA0n0Ouwre7z9kn7trV/7kuitXV3praff2QF0T6/37AC6hLADSfQk7LYvs/2M7W22V/Sihyq2t9ve3JqGeqjHvayyvcf2lhHLZtteZ/u51u2oc+z1qLe+mMa7MM14T9+7Xk9/3vXv7LYnS3pW0gcl7ZC0QdLSiHi6q41UsL1d0kBE9PwADNvvl/SypK9GxLtby/5J0t6IuKP1H+WsiPjbPuntdkkv93oa79ZsRfNHTjMu6WpJf6oevneFvq5VF963XuzZL5C0LSKej4iDkh6UdFUP+uh7EfGYpL3HLb5K0urW/dUa/sfSdRW99YWI2BURT7Tu75d0bJrxnr53hb66ohdhP0vSj0c83qH+mu89JD1i+3Hby3vdzCjmRsQuafgfj6Qze9zP8cacxrubjptmvG/eu3amP6+rF2EfbSqpfhr/uygi3ivpckk3tj6uYnzGNY13t4wyzXhfaHf687p6EfYdkhaOeLxA0s4e9DGqiNjZut0j6SH131TUu4/NoNu63dPjfn6tn6bxHm2acfXBe9fL6c97EfYNkhbZfqvtaZKuk7SmB328ju0ZrR9OZHuGpA+p/6aiXiNpWev+MkkP97CX1+iXabyrphlXj9+7nk9/HhFd/5N0hYZ/kf+BpFt70UNFX2+T9GTr76le9ybpAQ1/rDuk4U9EN0g6XdKgpOdat7P7qLf7JG2WtEnDwZrfo96WaPir4SZJG1t/V/T6vSv01ZX3jcNlgSQ4gg5IgrADSRB2IAnCDiRB2IEkCDuQBGEHkvh/CHVJRcsiU00AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[0].view(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bare Training Loop\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(nn.Module):\n",
    "    \n",
    "    def __init__(self, x_dim, y_dim, h_dim):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(x_dim, h_dim),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Linear(h_dim, y_dim)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Config:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, m = x_train.shape\n",
    "c = (y_train.max() - y_train.min()+1).item()\n",
    "n, m, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FC(m, c, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Config:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 1e-3\n",
    "bs = 128\n",
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    num_batches = (n-1)//bs\n",
    "    for i in range(num_batches):\n",
    "        # data minibatch\n",
    "        start_idx, end_idx = bs*i, bs*(i+1)\n",
    "        xb = x_train[start_idx:end_idx]\n",
    "        yb = y_train[start_idx:end_idx]\n",
    "        # forward pass \n",
    "        yb_pred = model(xb)\n",
    "        loss = loss_fn(yb_pred, yb)\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        # update model\n",
    "        with torch.no_grad():\n",
    "            for layer in model.layers:\n",
    "                if hasattr(layer, \"weight\"):\n",
    "                    layer.weight -= lr * layer.weight.grad\n",
    "                    layer.weight.grad.zero_()\n",
    "                if hasattr(layer, \"bias\"):\n",
    "                    layer.bias -= lr * layer.bias.grad\n",
    "                    layer.bias.grad.zero_()       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(input, target):\n",
    "    input_pred = torch.argmax(input, dim=-1)\n",
    "    return (input_pred==target).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4428, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(model(x_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8839)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model(x_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Data Abstractions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "**A simple wrapper for x's and y's**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x, self.y = x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop using Dataset abstraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(x_train, y_train)\n",
    "test_ds = Dataset(x_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    num_batches = (n-1)//bs\n",
    "    for i in range(num_batches):\n",
    "        # data minibatch\n",
    "        xb,yb = train_ds[bs*i:bs*(i+1)]\n",
    "        # forward pass \n",
    "        yb_pred = model(xb)\n",
    "        loss = loss_fn(yb_pred, yb)\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        # update model\n",
    "        with torch.no_grad():\n",
    "            for layer in model.layers:\n",
    "                if hasattr(layer, \"weight\"):\n",
    "                    layer.weight -= lr * layer.weight.grad\n",
    "                    layer.weight.grad.zero_()\n",
    "                if hasattr(layer, \"bias\"):\n",
    "                    layer.bias -= lr * layer.bias.grad\n",
    "                    layer.bias.grad.zero_()       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9029)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model(x_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "**Loading minibatches from a Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.ds, self.bs = dataset, batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.ds), self.bs):\n",
    "            yield self.ds[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(range(10))\n",
    "y = x*x\n",
    "dummy_ds = Dataset(x,y)\n",
    "dummy_dl = DataLoader(dummy_ds, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([0., 1.]), tensor([0., 1.])),\n",
       " (tensor([2., 3.]), tensor([4., 9.])),\n",
       " (tensor([4., 5.]), tensor([16., 25.])),\n",
       " (tensor([6., 7.]), tensor([36., 49.])),\n",
       " (tensor([8., 9.]), tensor([64., 81.]))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(xb,yb) for xb,yb in dummy_dl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop using DataLoader abstraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(x_train, y_train)\n",
    "test_ds = Dataset(x_train, y_test)\n",
    "\n",
    "train_dl = DataLoader(train_ds, bs)\n",
    "test_dl = DataLoader(test_ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    # data minibatch\n",
    "    for xb,yb in train_dl:\n",
    "        # forward pass \n",
    "        yb_pred = model(xb)\n",
    "        loss = loss_fn(yb_pred, yb)\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        # update model\n",
    "        with torch.no_grad():\n",
    "            for layer in model.layers:\n",
    "                if hasattr(layer, \"weight\"):\n",
    "                    layer.weight -= lr * layer.weight.grad\n",
    "                    layer.weight.grad.zero_()\n",
    "                if hasattr(layer, \"bias\"):\n",
    "                    layer.bias -= lr * layer.bias.grad\n",
    "                    layer.bias.grad.zero_()       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9130)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model(x_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sampler\n",
    "**Shuffles data during batch training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    \n",
    "    def __init__(self, data_size, batch_size, shuffle):\n",
    "        self.n, self.bs, self.shuffle = data_size, batch_size, shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        sample_idx = torch.randperm(self.n) if self.shuffle else list(range(self.n))\n",
    "        for i in range(0, self.n, self.bs):\n",
    "            yield sample_idx[i : i+self.bs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(range(10))\n",
    "y = x*x\n",
    "dummy_samp = Sampler(10, 2, False)\n",
    "print([samples for samples in dummy_samp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([4, 1]), tensor([8, 3]), tensor([6, 5]), tensor([2, 0]), tensor([7, 9])]\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(range(10))\n",
    "y = x*x\n",
    "dummy_samp = Sampler(10, 2, True)\n",
    "print([samples for samples in dummy_samp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataLoader with shuffling feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self, dataset, batch_size, sampler, collate_fn):\n",
    "        self.ds, self.bs = dataset, batch_size        \n",
    "        self.sampler, self.collate_fn = sampler, collate_fn\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch_idxs in self.sampler:\n",
    "            yield self.collate_fn([self.ds[idx] for idx in batch_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    return torch.stack(xs), torch.stack(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([0., 1.]), tensor([0., 1.])),\n",
       " (tensor([2., 3.]), tensor([4., 9.])),\n",
       " (tensor([4., 5.]), tensor([16., 25.])),\n",
       " (tensor([6., 7.]), tensor([36., 49.])),\n",
       " (tensor([8., 9.]), tensor([64., 81.]))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor(range(10))\n",
    "y = x*x\n",
    "\n",
    "dummy_ds =  Dataset(x, y)\n",
    "dummy_samp = Sampler(10, 2, False)\n",
    "dummy_dl = DataLoader(dummy_ds, bs, dummy_samp, collate)\n",
    "\n",
    "[batch for batch in dummy_dl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([9., 8.]), tensor([81., 64.])),\n",
       " (tensor([2., 5.]), tensor([ 4., 25.])),\n",
       " (tensor([4., 6.]), tensor([16., 36.])),\n",
       " (tensor([7., 0.]), tensor([49.,  0.])),\n",
       " (tensor([1., 3.]), tensor([1., 9.]))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor(range(10))\n",
    "y = x*x\n",
    "\n",
    "dummy_ds =  Dataset(x, y)\n",
    "dummy_samp = Sampler(10, 2, True)\n",
    "dummy_dl = DataLoader(dummy_ds, bs, dummy_samp, collate)\n",
    "\n",
    "[batch for batch in dummy_dl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop with shuffled DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(x_train, y_train, x_test, y_test, bs, collate):\n",
    "    train_ds = Dataset(x_train, y_train)\n",
    "    test_ds = Dataset(x_test, y_test)\n",
    "\n",
    "    train_samp = Sampler(len(train_ds), bs, shuffle=True)\n",
    "    test_samp = Sampler(len(test_ds), bs, shuffle=False)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, bs, train_samp, collate)\n",
    "    test_dl = DataLoader(test_ds, bs, test_samp, collate)\n",
    "    \n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl = get_dataloaders(x_train, y_train,\n",
    "                                    x_test, y_test,\n",
    "                                    bs, collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    # data minibatch\n",
    "    for xb,yb in train_dl:\n",
    "        # forward pass \n",
    "        yb_pred = model(xb)\n",
    "        loss = loss_fn(yb_pred, yb)\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        # update model\n",
    "        with torch.no_grad():\n",
    "            for layer in model.layers:\n",
    "                if hasattr(layer, \"weight\"):\n",
    "                    layer.weight -= lr * layer.weight.grad\n",
    "                    layer.weight.grad.zero_()\n",
    "                if hasattr(layer, \"bias\"):\n",
    "                    layer.bias -= lr * layer.bias.grad\n",
    "                    layer.bias.grad.zero_()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9200)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model(x_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Training Abstractions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Parameters\n",
    "**All trainable parameters of the model are abstracted together**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How PyTorch does it?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, x_dim, y_dim, h_dim):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(x_dim, h_dim)\n",
    "        self.l2 = nn.Linear(h_dim, y_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        return self.l2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (l1): Linear(in_features=784, out_features=300, bias=True)\n",
       "  (l2): Linear(in_features=300, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(784,10,300)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl = get_dataloaders(x_train, y_train, x_test, y_test, bs, collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for xb,yb in train_dl:\n",
    "        yb_pred = model(xb)\n",
    "        loss = loss_fn(yb_pred, yb)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for p in model.parameters():\n",
    "                p -= lr * p.grad\n",
    "                p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9030)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model(x_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to rebuild this functionality?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurModule():\n",
    "    \n",
    "    def __init__(self, x_dim, y_dim, h_dim):\n",
    "        self._modules = {}\n",
    "        self.l1 = nn.Linear(x_dim, h_dim)\n",
    "        self.l2 = nn.Linear(h_dim, y_dim)\n",
    "    \n",
    "    def __setattr__(self, k, v):\n",
    "        if not k.startswith('_'):\n",
    "            self._modules[k] = v\n",
    "        super().__setattr__(k, v)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self._modules}\"\n",
    "    \n",
    "    def parameters(self):\n",
    "        for m in self._modules.values():\n",
    "            for p in m.parameters():\n",
    "                yield p\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        return self.l2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': Linear(in_features=784, out_features=300, bias=True), 'l2': Linear(in_features=300, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OurModule(784,10,300)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl = get_dataloaders(x_train, y_train, x_test, y_test, bs, collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for xb,yb in train_dl:\n",
    "        yb_pred = model(xb)\n",
    "        loss = loss_fn(yb_pred, yb)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for p in model.parameters():\n",
    "                p -= lr * p.grad\n",
    "                p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8843)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model(x_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Sequential\n",
    "**Quick model construction by defining a list of layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How PyTorch does it?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=300, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=300, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784,300), nn.ReLU(), nn.Linear(300,10))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to rebuild this functionality?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(nn.Module):\n",
    "    \n",
    "    def __init__(self, *layers):\n",
    "        super().__init__()\n",
    "        self.layers = self.module_list(layers)\n",
    "    \n",
    "    def module_list(self, layers):\n",
    "        for i, l in enumerate(layers):\n",
    "            self.add_module(name = f'{i}',\n",
    "                            module = l)\n",
    "    \n",
    "    def forward(self):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=300, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=300, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential(nn.Linear(784,300), nn.ReLU(), nn.Linear(300,10))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "**Abstracting away the optimization function used to train a model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How PyTorch does it?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=300, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=300, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784,300), nn.ReLU(), nn.Linear(300,10))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl = get_dataloaders(x_train, y_train, x_test, y_test, bs, collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for xb,yb in train_dl:\n",
    "        yb_pred = model(xb)\n",
    "        loss = loss_fn(yb_pred, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9995)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model(x_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to rebuild this functionality?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    \n",
    "    def __init__(self, params, lr):\n",
    "        self.params, self.lr = list(params), lr\n",
    "    \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params:\n",
    "                p -= lr * p.grad\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=300, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=300, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784,300), nn.ReLU(), nn.Linear(300,10))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl = get_dataloaders(x_train, y_train, x_test, y_test, bs, collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(model.parameters(), lr=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for xb,yb in train_dl:\n",
    "        yb_pred = model(xb)\n",
    "        loss = loss_fn(yb_pred, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8854)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model(x_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=300, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=300, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784,300), nn.ReLU(), nn.Linear(300,10))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl = get_dataloaders(x_train, y_train, x_test, y_test, bs, collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training & Validation Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH#:0 \t Loss: 0.0623 Accuracy: 0.7748\n",
      "EPOCH#:1 \t Loss: 0.0630 Accuracy: 0.7746\n",
      "EPOCH#:2 \t Loss: 0.0629 Accuracy: 0.7751\n",
      "EPOCH#:3 \t Loss: 0.0635 Accuracy: 0.7746\n",
      "EPOCH#:4 \t Loss: 0.0633 Accuracy: 0.7748\n",
      "EPOCH#:5 \t Loss: 0.0636 Accuracy: 0.7750\n",
      "EPOCH#:6 \t Loss: 0.0639 Accuracy: 0.7751\n",
      "EPOCH#:7 \t Loss: 0.0644 Accuracy: 0.7753\n",
      "EPOCH#:8 \t Loss: 0.0642 Accuracy: 0.7751\n",
      "EPOCH#:9 \t Loss: 0.0648 Accuracy: 0.7750\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Train model\n",
    "    model.train()\n",
    "    for xb,yb in train_dl:\n",
    "        yb_pred = model(xb)\n",
    "        loss = loss_fn(yb_pred, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    # Validate model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss, acc, n = 0., 0., 0.\n",
    "        for xb, yb in test_dl:\n",
    "            yb_pred = model(xb)\n",
    "            loss += loss_fn(yb_pred, yb)\n",
    "            acc += accuracy(yb_pred, yb)\n",
    "            n += len(xb)\n",
    "        loss = loss/n * 100\n",
    "        acc = acc/n * 100\n",
    "    print(f\"EPOCH#:{epoch} \\t Loss: {loss:.4f} Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:prod] *",
   "language": "python",
   "name": "conda-env-prod-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
